---
title: "A predictive modeling case study"
weight: 5
tags: [parsnip, recipes, rsample, tune]
categories: [model fitting, tuning]
description: | 
  Develop, from beginning to end, a predictive model using best practices.
---

```{r setup, include = FALSE, message = FALSE, warning = FALSE}
source(here::here("content/start/common.R"))
```

```{r load, include = FALSE, message = FALSE, warning = FALSE}
library(readr)
library(vip)
library(tidymodels)
pkgs <- c("tidymodels", "readr", "glmnet", "ranger", "vip")
theme_set(theme_bw() + theme(legend.position = "top"))
```

`r req_pkgs(pkgs)`

# Introduction

The previous _Getting Started_ articles have been focused on single tasks related to modeling. This example is a broader case study of building a predictive model from beginning to end. It uses all of the previous topics.  

Our modeling goal here is to use a data set of hotel stays and predict which hotel stays include children (vs. do not include children or babies), based on the other characteristics of the stays such as which hotel the guests stay at, how much they pay, etc. The [paper that this data comes from](https://www.sciencedirect.com/science/article/pii/S2352340918315191) points out that the distribution of many of these variables (such as number of adults/children, room type, meals bought, country of origin of the guests, and so forth) is different for canceled vs. not canceled hotel bookings. This is mostly because more information is gathered when guests check in; the biggest contributor to these differences is not that people who cancel are different from people who do not.

To build our models, we filtered the data to only the bookings that did not cancel and will build a model to predict which non-canceled hotel stays include children and which do not.

# Data Spending

The [version](https://gist.github.com/topepo/05a74916c343e57a71c51d6bc32a21ce) of the data that we'll use can be accessed from the `tidymodels.org` site: 

```{r hotel-import, message = FALSE}
library(tidymodels)
library(readr)

hotels <- 
  read_csv('https://bit.ly/hotel_booking_data') %>%
  mutate_if(is.character, as.factor) 

dim(hotels)
```

An important consideration for these data isthat children were only in `r round(mean(hotels$children == "children") * 100, 1)`% of the reservations. This type of severe class imbalance can often wreak havoc on an analysis. While there are several methods for combating this issue, the analyses shown below analyze the data as-is. 

For a data splitting strategy, 25% of the reservations were allocated to the test set using a stratified random sample:  

```{r tr-te-split}
set.seed(123)
splits <- initial_split(hotels, strata = children)

hotel_other <- training(splits)
hotel_test  <- testing(splits)
nrow(hotel_test)
```

Rather than using multiple iterations of resampling, a single _validation set_ will be split apart from the `r format(nrow(hotel_other), big.mark = ",")` reservations that were not used for testing. In tidymodels, a validation set is treated as a single iteration of resampling. For this reason, the `validation_split()` function was used to allocate 20% of these to validation and `r format(nrow(hotel_other) * .8, big.mark = ",")` reservations to the training set:  

```{r validation-set}
set.seed(234)
val_set <- validation_split(hotel_other, strata = children, prop = 0.80)
val_set
```

The same functions from the tune package will be used as in previous articles but, in this case, the performance metrics will be computed on a single set of `r format(nrow(hotel_other) * .2, big.mark = ",")` reservations. This amount of data should provide enough precision to be a reliable indicator for how well each model predicts the outcome.  

# A first model: logistic regression

It makes sense to start with a simple model. Since the outcome is categorical, a logistic regression model would be a good first step. Specifically, let's use a glmnet model to potentially perform feature selection during model training. This method of estimating the logistic regression slope parameters uses a _penalty_ on the process so that less relevant predictors are driven towards a value of zero. One of the glmnet penalization methods, called the lasso method, can set the predictor slopes to absolute zero if a large enough penalty is used. 

To specify a penalized logistic regression model that uses a feature selection penalty:  

```{r logistic-model}
lr_mod <- 
  logistic_reg(penalty = tune(), mixture = 1) %>% 
  set_engine("glmnet")
```

Setting `mixture` to a value of one means that the glmnet model will focus on potentially removing irrelevant predictors. 

To prepare the data for the model, the categorical predictors (e.g., `distribution_channel`, `hotel`, ...) should be converted to dummy variables. Additionally, it might make sense to create a set of date-based predictors that reflect important components related to the arrival date. First, `step_date()` creates predictors for the year, month, and day of the week. Secondly, `step_holiday()` generates a set of indicator variables for specific holidays. The addition of `step_zv()` means that no indicator variables that only contains a single unique value (e.g. all zeros) will be added to the model. This is important because, for penalized models, the the predictors should be centered and scaled. 

The recipe for these steps is: 

```{r logistic-features}
holidays <- c("AllSouls", "AshWednesday", "ChristmasEve", "Easter", 
              "ChristmasDay", "GoodFriday", "NewYearsDay", "PalmSunday")

lr_recipe <- 
  recipe(children ~ ., data = hotel_other) %>% 
  step_date(arrival_date) %>% 
  step_holiday(arrival_date, holidays = holidays) %>% 
  step_rm(arrival_date) %>% 
  step_dummy(all_nominal(), -all_outcomes()) %>% 
  step_zv(all_predictors()) %>% 
  step_normalize(all_predictors())
```

The model and recipe can be bundled into a single `workflow()` object to make management of the R objects easier:

```{r logistic-workflow}
lr_workflow <- 
  workflow() %>% 
  add_model(lr_mod) %>% 
  add_recipe(lr_recipe)
```

Finally a grid of penalty values are created and the model is tuned. The validation set predictions are saved (via the call to `control_grid()` below) so that diagnostic information can be available after the model fit. Also, the area under the ROC curve is used to quantify how well the model performs across a continuum of event thresholds (recall that the event rate is very low for these data). 


```{r logistic-fit, cache = TRUE}
lr_reg_grid <- expand.grid(penalty = 10^seq(-4, -1, length.out = 30))

tune_ctrl <- control_grid(save_pred = TRUE)
roc_only <- metric_set(roc_auc)

lr_res <- 
  lr_workflow %>% 
  tune_grid(val_set,
            grid = lr_reg_grid,
            control = tune_ctrl,
            metrics = roc_only)
```

The resulting validation set metrics are computed and plotted against the amount of penalization: 

```{r logistic-results, fig.height = 4.25, , fig.width = 6}
lr_res %>% 
  collect_metrics() %>% 
  ggplot(aes(x = penalty, y = mean)) + 
  geom_point() + 
  geom_line() + 
  ylab("Area under the ROC Curve") +
  scale_x_log10()
```

Performance is generally better when very little penalization is used; this suggests that the majority of the predictors are important to the model. Note the steep drop in the area under the ROC curve that occurs when the amount of penalization is high; this happens because a large enough penalty will remove _all_ predictors from the model. 


Since there is a plateau of performance for small penalties, a value closer to the decline in performance is chosen as being best for this model: 

```{r logistic-best}
lr_best <- 
  lr_res %>% 
  collect_metrics() %>% 
  arrange(desc(mean)) %>% 
  slice(8)
lr_best
```

This value has effectively the same performance as the numerically best, but might eliminate more predictors. For this specific penalty value, the validation set ROC curve is:

```{r logistic-roc-curve}
lr_auc <- 
  lr_res %>% 
  collect_predictions(parameters = lr_best) %>% 
  roc_curve(children, .pred_children) %>% 
  mutate(model = "Logistic Regression")

autoplot(lr_auc)
```

The level of performance generated by this logistic regression model is good but not groundbreaking. Perhaps the linear nature of the prediction equation is too limiting for this data set. 

As a next step, we might consider a highly non-linear model generated using tree-based methods. 

# Tree-based ensembles

One effective and low-maintenance modeling technique is a _random forest_ (also used in the [resampling article](/start/resampling/)). This model can be used with less preprocessing than the logistic regression; conversion to dummy variables and variable normalization are not required. As before, the date predictor is engineered so that the random forest model does not need to work hard to tease these potential patterns from the data.  

```{r rf-features}
rf_recipe <- 
  recipe(children ~ ., data = hotel_other) %>% 
  step_date(arrival_date) %>% 
  step_holiday(arrival_date) %>% 
  step_rm(arrival_date) 
```

The computations required for model tuning can usually be easily parallelized. However, when the models are resampled, the most efficient approach is to parallelize the resampling process. In this case study, a single validation set is being used so parallelization isn't an option using the tune package. Despite this, the ranger package can compute the individual random forest models in parallel. To do this, the number of cores to use should be specified. To determine this, the parallel package can be used to understand how much parallelization can be done on your specific computer: 

```{r num-cores}
cores <- parallel::detectCores()
cores
```

To declare that parallel processing should be used, the `num.threads` argument for `ranger::ranger()` can be passed when setting the computational engine: 

```{r rf-model}
rf_mod <- 
  rand_forest(mtry = tune(), min_n = tune(), trees = 1000) %>% 
  set_engine("ranger", num.threads = cores) %>% 
  set_mode("classification")

rf_workflow <- 
  workflow() %>% 
  add_model(rf_mod) %>% 
  add_recipe(rf_recipe)
```

Again, if any other resampling method were used, it is better to parallel process in the more usual way. 

To tune, a space-filling design with 25 candidate models is used: 

```{r rf-fit, cache = TRUE}
set.seed(345)
rf_res <- 
  rf_workflow %>% 
  tune_grid(val_set,
            grid = 25,
            control = tune_ctrl,
            metrics = roc_only)
```

The note about "finalizing the unknown parameter" is related to the size of the data set. Since `mtry` depends on the number of predictors in the data set, `tune_grid()` determines the upper bound for `mtry` once it receives the data. 

The results of the tuning process, when plotted, indicate that both `mtry` and the minimum number of data points required to keep splitting should be fairly small (on average). Note, however, that the range of the y-axis indicates that the model is very robust to the choice of these parameters. 

```{r rf-results, fig.height = 4}
autoplot(rf_res)
```

If the model with the numerically best results are used, the final tuning parameter values would be:

```{r rf-best}
rf_best <- select_best(rf_res, metric = "roc_auc")
rf_best
```

As before, the validation set ROC curve can be produced and overlaid with the previous logistic regression model: 

```{r rf-roc-curve}
rf_auc <- 
  rf_res %>% 
  collect_predictions(parameters = rf_best) %>% 
  roc_curve(children, .pred_children) %>% 
  mutate(model = "Random Forest")

bind_rows(rf_auc, lr_auc) %>% 
  ggplot(aes(x = 1 - specificity, y = sensitivity, col = model)) + 
  geom_path() +
  geom_abline(lty = 2) + 
  coord_equal() + 
  scale_color_brewer(palette = "Set1")
```

The random forest is uniformly better across event probability thresholds. 

If this model were chosen to be better than the other models, it can be used once again with `last_fit()` to fit the final model and then evaluate the test set. 

However, the model object is redefined so that the _variable importance_ scores are computed for this model. This should give some insight into which predictors are driving model performance.

```{r rf-final, cache = TRUE}
rf_mod <- 
  rand_forest(mtry = 5, min_n = 3, trees = 1000) %>% 
  set_engine("ranger", num.threads = cores, importance = 'impurity') %>% 
  set_mode("classification")

rf_workflow <- 
  workflow() %>% 
  add_model(rf_mod) %>% 
  add_recipe(rf_recipe)

rf_fit <- rf_workflow %>% last_fit(splits)
```

From this fitted workflow, the vip package can be used to visualize the results: 

```{r rf-importance}
library(vip)

rf_fit$.workflow %>% 
  pluck(1) %>% 
  pull_workflow_fit() %>% 
  vip(num_features = 20) 
```

The most important predictors in whether a hotel stay had children or not were the daily cost for the room, the type of reservation, the time between the creation of the reservation and the arrival date, and the type of room that was reserved. 

# Test set results

How did this model do on the test set? Was the validation set a good estimate of future performance? 

```{r test-set-roc-curve}
rf_fit %>% 
  collect_predictions() %>% 
  roc_auc(children, .pred_children)

rf_fit %>% 
  collect_predictions() %>% 
  roc_curve(children, .pred_children) %>% 
  autoplot()
```

Based on these results, the validation set and test set performance statistics are very close. 

# Session information

```{r si, echo = FALSE}
small_session(pkgs)
```


